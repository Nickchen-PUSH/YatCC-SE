"""Kubernetes é›†ç¾¤å®ç°

ä¸“é—¨é’ˆå¯¹ç”¨æˆ·ç‹¬ç«‹çš„ code-server é•œåƒçš„éƒ¨ç½²å’Œç®¡ç†ã€‚
"""

import asyncio as aio
import uuid
import threading
import socket
import subprocess
from base.logger import logger
from config import CONFIG
from datetime import datetime
from typing import List, Optional, Dict
from kubernetes.client.rest import ApiException

from . import (
    ClusterABC, JobParams, JobInfo,
    ClusterError, JobNotFoundError
)

LOGGER = logger(__spec__, __file__)


class KubernetesCluster(ClusterABC):
    """Kubernetes é›†ç¾¤å®ç°"""

    def __init__(self, config=None):
        super().__init__(config)
        self._k8s_client = None
        self._apps_v1 = None
        self._core_v1 = None
        self._networking_v1 = None
        self._is_initialized = False
        self._port_forwards: Dict[str, Dict] = {}
        self._local_port_base = 30000  # æœ¬åœ°ç«¯å£åŸºæ•°ï¼Œç”¨äºç«¯å£è½¬å‘
    
    async def initialize(self):
        """åˆå§‹åŒ– Kubernetes é›†ç¾¤è¿æ¥"""
        if self._is_initialized:
            return
        
        try:
            # å¯¼å…¥ Kubernetes å®¢æˆ·ç«¯
            from kubernetes import client, config as k8s_config
            
            # å°è¯•åŠ è½½é…ç½®
            try:
                if self.config.Kubernetes.KUBECONFIG_PATH:
                    k8s_config.load_kube_config(config_file=self.config.Kubernetes.KUBECONFIG_PATH)
                else:
                    # å°è¯•è‡ªåŠ¨åŠ è½½é…ç½®
                    k8s_config.load_kube_config()
                LOGGER.info("Loaded kubeconfig from file")
            except Exception:
                try:
                    # å¦‚æœåœ¨é›†ç¾¤å†…ï¼Œå°è¯•åŠ è½½é›†ç¾¤å†…é…ç½®
                    k8s_config.load_incluster_config()
                    LOGGER.info("Loaded in-cluster config")
                except Exception as e:
                    raise ClusterError(f"Failed to load Kubernetes config: {e}")
            
            # åˆ›å»ºå®¢æˆ·ç«¯
            self._k8s_client = client.ApiClient()
            self._apps_v1 = client.AppsV1Api()
            self._core_v1 = client.CoreV1Api()
            self._networking_v1 = client.NetworkingV1Api()
            
            # æµ‹è¯•è¿æ¥
            await aio.to_thread(self._core_v1.list_namespace, timeout_seconds=10)
            
            self._is_initialized = True
            LOGGER.info("Kubernetes cluster initialized successfully")
            
        except Exception as e:
            LOGGER.error(f"Failed to initialize Kubernetes cluster: {e}")
            raise ClusterError(f"Kubernetes initialization failed: {e}")
    
    async def ensure_initialized(self):
        """ç¡®ä¿å·²åˆå§‹åŒ–"""
        if not self._is_initialized:
            await self.initialize()

    @property
    def apps_v1(self):
        """è·å– AppsV1Api å®¢æˆ·ç«¯"""
        return self._apps_v1
    
    @property
    def core_v1(self):
        """è·å– CoreV1Api å®¢æˆ·ç«¯"""
        return self._core_v1

    async def allocate_resources(self, job_params):
        """åˆ›å»ºä½œä¸š"""
        await self.ensure_initialized()
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not job_params.user_id:
            raise ValueError("user_id is required")
        
        return await self._create_new_deployment(job_params)        

    async def submit_job(self, job_params: JobParams) -> JobInfo:
        """æäº¤ code-server ä½œä¸šåˆ° Kubernetes"""
        await self.ensure_initialized()

        # éªŒè¯å¿…éœ€å‚æ•°
        if not job_params.name:
            raise ValueError("job_name is required")
        
        existing_job = await self._find_user_deployment(job_params)

        if existing_job:
            return await self._resume_user_deployment(existing_job, job_params)
        else:
            LOGGER.info("No existing deployment found for user {job_params.user_id}, creating a new one.")
            return await self._create_new_deployment(job_params)

    async def _find_user_deployment(self, job_params: JobParams) -> Optional[JobInfo]:
        """æŸ¥æ‰¾ç”¨æˆ·çš„ç°æœ‰ Deployment"""
        try:
            deployments = await aio.to_thread(
                self.apps_v1.list_namespaced_deployment,
                namespace = self.config.Kubernetes.NAMESPACE,
                label_selector=f"managed-by=yatcc-se,user-id={job_params.user_id},type=code-server"
            )

            if deployments.items:
                deployment = deployments.items[0]
                job_name = deployment.metadata.name
                LOGGER.info(f"Found existing deployment for user {job_params.user_id}: {job_name}")
                return job_name

            return None
        except ApiException as e:
            LOGGER.warning(f"Error finding user deployment: {e}")
            return None
        
    async def _create_new_deployment(self, job_params: JobParams) -> JobInfo:
        """åˆ›å»ºç”¨æˆ· Deployment"""
        # ç”Ÿæˆå”¯ä¸€ä½œä¸šåç§°
        job_name = self._generate_job_name(job_params)
        
        try:
            # åˆ›å»ºæŒä¹…åŒ–å­˜å‚¨
            # await self._create_pvc(job_name, job_params)
            
            # åˆ›å»º Deployment
            await self._create_deployment(job_name, job_params)
            
            # åˆ›å»º Service
            service_url = await self._create_service(job_name, job_params)
            
            return JobInfo(
                id=job_name,
                name=job_params.name,
                image=job_params.image,
                ports=job_params.ports,
                env=job_params.env,
                status=JobInfo.Status.PENDING,
                created_at=datetime.now().isoformat(),
                namespace=self.config.Kubernetes.NAMESPACE,
                service_url=service_url,
                user_id=job_params.user_id,
            )
            
        except Exception as e:
            LOGGER.error(f"Failed to submit job: {e}")
            # æ¸…ç†å¯èƒ½åˆ›å»ºçš„èµ„æº
            await self._cleanup_job_resources(job_name)
            raise ClusterError(f"Failed to submit job: {e}")

    async def _resume_user_deployment(self, job_name: str, job_params: JobParams) -> JobInfo:
        """æ¢å¤ç”¨æˆ·çš„ Deployment"""
        try:
            deployment = await aio.to_thread(
                self.apps_v1.read_namespaced_deployment,
                name=job_name,
                namespace=self.config.Kubernetes.NAMESPACE
            )
            
            # æ£€æŸ¥æ˜¯å¦è¢«æš‚åœ
            annotations = deployment.metadata.annotations or {}
            is_suspended = annotations.get("yatcc-se/suspended") == "true"
            
            logger.info(f"Checking deployment {job_name} suspension status:")
            logger.info(f"  - Annotations: {annotations}")
            logger.info(f"  - Is suspended: {is_suspended}")
            
            if is_suspended:
                # æ¢å¤ Deployment
                logger.info(f"Unsuspending deployment: {job_name}")
                await self._unsuspend_deployment(job_name)
                logger.info(f"Resumed suspended deployment: {job_name}")
                
                # éªŒè¯æ³¨è§£æ˜¯å¦è¢«ç§»é™¤
                updated_annotations = deployment.metadata.annotations or {}
                LOGGER.info(f"After unsuspend, annotations: {updated_annotations}")
            else:
                # Deployment å·²ç»åœ¨è¿è¡Œ
                logger.info(f"Deployment {job_name} is already running")
            
            # ğŸ¯ æ›´æ–°ç¯å¢ƒå˜é‡å’Œé…ç½®ï¼ˆå¦‚æœæœ‰å˜åŒ–ï¼‰
            await self._update_deployment_if_needed(job_name, job_params)
            
            # ç¡®ä¿ Service å­˜åœ¨
            service_url = await self._ensure_service_exists(job_name, job_params)
            
            # å†æ¬¡è¯»å–æœ€æ–°çš„ deployment çŠ¶æ€
            final_deployment = await aio.to_thread(
                self.apps_v1.read_namespaced_deployment,
                name=job_name,
                namespace=self.config.Kubernetes.NAMESPACE
            )
            
            # è¿”å› JobInfo
            return await self._build_job_info_from_deployment(final_deployment, service_url)
            
        except ApiException as e:
            if e.status == 404:
                # Deployment ä¸å­˜åœ¨äº†ï¼Œåˆ›å»ºæ–°çš„
                logger.warning(f"Deployment {job_name} not found, creating new one")
                return await self._create_new_deployment(job_params)
            raise ClusterError(f"Failed to resume deployment: {e}")

    async def _unsuspend_deployment(self, job_name: str) -> None:
        """æ¢å¤è¢«æš‚åœçš„ Deployment"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"Unsuspending deployment {job_name} (attempt {attempt + 1})")
                
                # é‡æ–°è¯»å–æœ€æ–°çš„ deployment
                deployment = await aio.to_thread(
                    self.apps_v1.read_namespaced_deployment,
                    name=job_name,
                    namespace=self.config.Kubernetes.NAMESPACE
                )
                
                annotations = deployment.metadata.annotations or {}
                LOGGER.info(f"Current annotations before unsuspend: {annotations}")
                
                # ä¿®å¤ï¼šæ­£ç¡®è·å–åŸå§‹å‰¯æœ¬æ•°
                original_replicas_str = annotations.get("yatcc-se/original-replicas", "1")
                try:
                    original_replicas = int(original_replicas_str)
                except ValueError:
                    LOGGER.warning(f"Invalid original-replicas value: {original_replicas_str}, using default 1")
                    original_replicas = 1
                
                # æ¢å¤å‰¯æœ¬æ•°
                deployment.spec.replicas = original_replicas
                LOGGER.info(f"Setting replicas to: {original_replicas}")
                
                # ğŸ¯ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ³¨è§£è¢«å®Œå…¨åˆ é™¤
                # åˆ›å»ºæ–°çš„æ³¨è§£å­—å…¸ï¼Œæ’é™¤æš‚åœç›¸å…³çš„æ³¨è§£
                new_annotations = {}
                for key, value in annotations.items():
                    if key not in ["yatcc-se/suspended", "yatcc-se/original-replicas"]:
                        new_annotations[key] = value
                
                # è®¾ç½®æ–°çš„æ³¨è§£å­—å…¸
                deployment.metadata.annotations = new_annotations
                
                LOGGER.info(f"New annotations after removal: {new_annotations}")
                LOGGER.info(f"Removed annotations: yatcc-se/suspended, yatcc-se/original-replicas")
                
                # æ‰§è¡Œæ›´æ–°
                updated_deployment = await aio.to_thread(
                    self.apps_v1.patch_namespaced_deployment,
                    name=job_name,
                    namespace=self.config.Kubernetes.NAMESPACE,
                    body=deployment
                )
                
                logger.info(f"Successfully unsuspended deployment: {job_name}")
                logger.info(f"Final replicas: {updated_deployment.spec.replicas}")
                logger.info(f"Final annotations: {updated_deployment.metadata.annotations}")
                return
                
            except ApiException as e:
                if e.status == 409 and attempt < max_retries - 1:
                    # ç‰ˆæœ¬å†²çªï¼Œç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    logger.warning(f"Deployment {job_name} version conflict, retrying... (attempt {attempt + 1})")
                    await aio.sleep(0.5 * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´
                    continue
                elif e.status == 404:
                    logger.warning(f"Deployment {job_name} not found during unsuspend")
                    return
                else:
                    raise ClusterError(f"Failed to unsuspend deployment: {e}")
            except Exception as e:
                logger.error(f"Unexpected error unsuspending {job_name}: {e}")
                raise ClusterError(f"Unexpected error unsuspending deployment: {e}")
    

    async def _update_deployment_if_needed(self, job_name: str, job_params: JobParams) -> bool:
        """å¦‚æœéœ€è¦ï¼Œæ›´æ–° Deployment çš„é…ç½®"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # é‡æ–°è¯»å–æœ€æ–°çš„ deployment
                deployment = await aio.to_thread(
                    self.apps_v1.read_namespaced_deployment,
                    name=job_name,
                    namespace=self.config.Kubernetes.NAMESPACE
                )
                
                container = deployment.spec.template.spec.containers[0]
                current_env = {env_var.name: env_var.value for env_var in (container.env or [])}
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç¯å¢ƒå˜é‡
                needs_update = False
                
                # æ¯”è¾ƒç¯å¢ƒå˜é‡
                for key, value in job_params.env.items():
                    if current_env.get(key) != value:
                        needs_update = True
                        break
                
                # æ¯”è¾ƒèµ„æºé™åˆ¶
                current_limits = container.resources.limits or {}
                if (current_limits.get("memory") != job_params.memory_limit or
                    current_limits.get("cpu") != job_params.cpu_limit):
                    needs_update = True
                
                if needs_update:
                    # æ›´æ–°ç¯å¢ƒå˜é‡
                    container.env = [{"name": k, "value": v} for k, v in job_params.env.items()]
                    
                    # æ›´æ–°èµ„æºé™åˆ¶
                    container.resources.limits = {
                        "memory": job_params.memory_limit or self.config.Codespace.DEFAULT_MEMORY_LIMIT,
                        "cpu": job_params.cpu_limit or self.config.Codespace.DEFAULT_CPU_LIMIT
                    }
                    
                    await aio.to_thread(
                        self.apps_v1.patch_namespaced_deployment,
                        name=job_name,
                        namespace=self.config.Kubernetes.NAMESPACE,
                        body=deployment
                    )
                    
                    logger.info(f"Updated deployment configuration: {job_name}")
                    return True
                
                return False
                
            except ApiException as e:
                if e.status == 409 and attempt < max_retries - 1:
                    # ç‰ˆæœ¬å†²çªï¼Œç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    logger.warning(f"Deployment {job_name} update conflict, retrying... (attempt {attempt + 1})")
                    await aio.sleep(0.5 * (attempt + 1))
                    continue
                elif e.status == 404:
                    logger.warning(f"Deployment {job_name} not found during update")
                    return False
                else:
                    logger.warning(f"Failed to update deployment {job_name}: {e}")
                    return False
        
        logger.warning(f"Failed to update deployment {job_name} after {max_retries} attempts")
        return False

    async def _ensure_service_exists(self, job_name: str, job_params: JobParams) -> str:
        """ç¡®ä¿ Service å­˜åœ¨"""
        try:
            service = await aio.to_thread(
                self.core_v1.read_namespaced_service,
                name=f"{job_name}-svc",
                namespace=self.config.Kubernetes.NAMESPACE
            )
            
            # Service å­˜åœ¨ï¼Œè¿”å› URL
            return f"http://{service.metadata.name}.{service.metadata.namespace}.svc.cluster.local:{service.spec.ports[0].port}"
            
        except ApiException as e:
            if e.status == 404:
                # Service ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                logger.info(f"Service not found for {job_name}, creating...")
                return await self._create_service(job_name, job_params)
            raise ClusterError(f"Failed to check service: {e}")

    async def _build_job_info_from_deployment(self, deployment, service_url: str) -> JobInfo:
        """ä» Deployment æ„å»º JobInfo"""
        container = deployment.spec.template.spec.containers[0]
        labels = deployment.metadata.labels or {}
        
        # ç¡®å®šçŠ¶æ€
        if deployment.status.ready_replicas and deployment.status.ready_replicas >= 1:
            status = JobInfo.Status.RUNNING
        elif deployment.status.unavailable_replicas:
            status = JobInfo.Status.FAILED
        else:
            status = JobInfo.Status.PENDING
        
        return JobInfo(
            id=deployment.metadata.name,
            name=deployment.metadata.labels.get("app", deployment.metadata.name),
            image=container.image,
            ports=[self.config.Codespace.PORT],
            env={env_var.name: env_var.value for env_var in (container.env or [])},
            status=status,
            created_at=deployment.metadata.creation_timestamp.isoformat() if deployment.metadata.creation_timestamp else None,
            namespace=deployment.metadata.namespace,
            service_url=service_url,
            user_id=int(labels.get("user-id", 0)) if labels.get("user-id") else None,
        )

    def _generate_job_name(self, job_params: JobParams) -> str:
        """ç”Ÿæˆä½œä¸šåç§°"""
        base_name = f"{job_params.user_id}"
        return base_name

    async def _create_deployment(self, job_name: str, job_params: JobParams):
        """åˆ›å»º Deployment"""
        # åˆå¹¶é»˜è®¤ç¯å¢ƒå˜é‡
        env_vars = {
            **job_params.env
        }
        
        deployment_spec = {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": job_name,
                "labels": self._build_labels(job_params),
                "namespace": self.config.Kubernetes.NAMESPACE
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {"app": job_name}
                },
                "template": {
                    "metadata": {
                        "labels": {"app": job_name, **self._build_labels(job_params)}
                    },
                    "spec": {
                        "containers": [{
                            "name": "code-server",
                            "image": job_params.image,
                            "ports": [{"containerPort": self.config.Codespace.PORT}],
                            "env": [{"name": k, "value": v} for k, v in env_vars.items()],
                            "args": [
                                "--bind-addr=0.0.0.0:8080",
                                "--auth=password",
                                "--disable-telemetry",
                                "/workspace"
                            ],
                            "volumeMounts": [
                                {
                                "name": "code",
                                "mountPath": "/workspace"
                                },
                                {
                                    "name": "io",
                                    "mountPath": "/io"
                                },
                                {
                                    "name": "root",
                                    "mountPath": "/data"
                                }
                            ],
                            "resources": {
                                "requests": {
                                    "memory": "256Mi",
                                    "cpu": "250m"
                                },
                                "limits": {
                                    "memory": job_params.memory_limit or self.config.Codespace.DEFAULT_MEMORY_LIMIT,
                                    "cpu": job_params.cpu_limit or self.config.Codespace.DEFAULT_CPU_LIMIT
                                },
                            },
                            "readinessProbe": {
                                "httpGet": {
                                    "path": "/",
                                    "port": self.config.Codespace.PORT
                                },
                                "initialDelaySeconds": 30,
                                "periodSeconds": 10
                            },
                            "livenessProbe": {
                                "httpGet": {
                                    "path": "/",
                                    "port": self.config.Codespace.PORT
                                },
                                "initialDelaySeconds": 60,
                                "periodSeconds": 30
                            }
                        }],
                        "volumes": [
                            {
                                "name": "code",
                                "hostPath": {
                                    "path": f"{CONFIG.CORE.students_dir}/{str(job_params.user_id)}/code",
                                    "type": "DirectoryOrCreate"
                                }
                            },
                            {
                                "name": "io",
                                "hostPath": {
                                    "path": f"{CONFIG.CORE.students_dir}/{str(job_params.user_id)}/io",
                                    "type": "DirectoryOrCreate"
                                }
                            },
                            {
                                "name": "root",
                                "hostPath": {
                                    "path": f"{CONFIG.CORE.students_dir}/{str(job_params.user_id)}/root",
                                    "type": "DirectoryOrCreate"
                                }
                            }
                        ],
                        "restartPolicy": "Always"
                    }
                }
            }
        }
        
        await aio.to_thread(
            self.apps_v1.create_namespaced_deployment,
            namespace=self.config.Kubernetes.NAMESPACE,
            body=deployment_spec
        )
        LOGGER.info(f"Created Deployment: {job_name}")

    async def _create_service(self, job_name: str, job_params: JobParams) -> str:
        """åˆ›å»º Serviceï¼Œè¿”å›å†…éƒ¨é›†ç¾¤ URL"""
        service_spec = {
            "apiVersion": "v1",
            "kind": "Service",
            "metadata": {
                "name": f"{job_name}-svc",
                "labels": self._build_labels(job_params),
                "namespace": self.config.Kubernetes.NAMESPACE
            },
            "spec": {
                "selector": {"app": job_name},
                "ports": [{
                    "port": self.config.Codespace.PORT,
                    "targetPort": self.config.Codespace.PORT,
                    "protocol": "TCP",
                    "name": "http"
                }],
                "type": "NodePort"
            }
        }
        
        service = await aio.to_thread(
            self.core_v1.create_namespaced_service,
            namespace=self.config.Kubernetes.NAMESPACE,
            body=service_spec
        )

        node_port = service.spec.ports[0].node_port
        service.nodeport = node_port

        # è¿”å›å†…éƒ¨é›†ç¾¤ URL
        service_url = f"http://{job_name}-svc.{self.config.Kubernetes.NAMESPACE}.svc.cluster.local:{self.config.CodeServer.PORT}"
        LOGGER.info(f"Created Service: {job_name}-svc, Internal URL: {service_url}")
        return service_url

    async def get_service_url(self, job_name: str) -> str:
        """è·å–æŒ‡å®šä½œä¸šçš„æœåŠ¡è®¿é—® URL - åˆ›å»ºç«¯å£è½¬å‘å¹¶è¿”å›æœ¬åœ° URL"""
        await self.ensure_initialized()
        
        try:
            # å…ˆæ£€æŸ¥ Service æ˜¯å¦å­˜åœ¨
            service = await aio.to_thread(
                self.core_v1.read_namespaced_service,
                name=f"{job_name}-svc",
                namespace=self.config.Kubernetes.NAMESPACE
            )
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç«¯å£è½¬å‘
            if job_name in self._port_forwards:
                forward_info = self._port_forwards[job_name]
                if forward_info['thread'].is_alive():
                    local_url = f"http://localhost:{forward_info['local_port']}"
                    LOGGER.debug(f"Using existing port forward for {job_name}: {local_url}")
                    return local_url
                else:
                    # æ¸…ç†æ— æ•ˆçš„ç«¯å£è½¬å‘
                    del self._port_forwards[job_name]
            
            # åˆ›å»ºæ–°çš„ç«¯å£è½¬å‘
            local_port = self._get_available_local_port()
            target_port = service.spec.ports[0].port
            
            # ä½¿ç”¨ kubectl port-forward åˆ›å»ºç«¯å£è½¬å‘
            success = await self._create_kubectl_port_forward(
                job_name=job_name,
                local_port=local_port,
                target_port=target_port
            )
            
            if success:
                local_url = f"http://localhost:{local_port}"
                LOGGER.info(f"Created port forward for {job_name}: {local_url} -> {target_port}")
                return local_url
            else:
                raise ClusterError(f"Failed to create port forward for {job_name}")
                
        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"Service not found for job: {job_name}")
            raise ClusterError(f"Failed to get service URL: {e}")

    async def _create_kubectl_port_forward(self, job_name: str, local_port: int, target_port: int) -> bool:
        """ä½¿ç”¨ kubectl port-forward åˆ›å»ºç«¯å£è½¬å‘"""
        import subprocess
        import signal
        
        def port_forward_worker():
            process = None
            try:
                service_name = f"{job_name}-svc"
                
                # æ„å»º kubectl port-forward å‘½ä»¤
                cmd = [
                    'kubectl', 'port-forward',
                    f'svc/{service_name}',
                    f'{local_port}:{target_port}',
                    '-n', self.config.Kubernetes.NAMESPACE
                ]
                
                LOGGER.info(f"Starting port forward: {' '.join(cmd)}")
                
                # å¯åŠ¨ç«¯å£è½¬å‘è¿›ç¨‹
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                # ä¿å­˜è¿›ç¨‹å¼•ç”¨
                forward_info = self._port_forwards.get(job_name, {})
                forward_info['process'] = process
                self._port_forwards[job_name] = forward_info
                
                # ç­‰å¾…è¿›ç¨‹ç»“æŸæˆ–åœæ­¢ä¿¡å·
                stop_event = forward_info.get('stop_event', threading.Event())
                
                # è¯»å–ä¸€äº›åˆå§‹è¾“å‡ºæ¥æ£€æŸ¥æ˜¯å¦æˆåŠŸå¯åŠ¨
                import select
                import time
                
                initial_output_read = False
                
                while not stop_event.is_set():
                    if process.poll() is not None:
                        # è¿›ç¨‹å·²ç»“æŸï¼Œè¯»å–æ‰€æœ‰è¾“å‡ºå¹¶å…³é—­æ–‡ä»¶å¥æŸ„
                        try:
                            stdout, stderr = process.communicate(timeout=5)
                            if stderr:
                                LOGGER.error(f"Port forward error for {job_name}: {stderr}")
                                if "pod is not running" in stderr:
                                    LOGGER.warning(f"Pod for {job_name} is not running yet")
                                elif "unable to forward port" in stderr:
                                    LOGGER.warning(f"Unable to forward port for {job_name}")
                        except subprocess.TimeoutExpired:
                            # å¼ºåˆ¶ç»ˆæ­¢å¹¶è¯»å–è¾“å‡º
                            process.kill()
                            stdout, stderr = process.communicate()
                            LOGGER.warning(f"Port forward process killed for {job_name}")
                        break
                    
                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å¾ªç¯ï¼Œå°è¯•è¯»å–ä¸€äº›åˆå§‹è¾“å‡º
                    if not initial_output_read:
                        time.sleep(2)  # ç»™è¿›ç¨‹ä¸€äº›å¯åŠ¨æ—¶é—´
                        initial_output_read = True
                    
                    time.sleep(1)
                
            except Exception as e:
                LOGGER.error(f"Port forward worker error for {job_name}: {e}")
            finally:
                # ç¡®ä¿è¿›ç¨‹è¢«æ­£ç¡®æ¸…ç†
                if process is not None:
                    try:
                        if process.poll() is None:
                            # è¿›ç¨‹ä»åœ¨è¿è¡Œï¼Œç»ˆæ­¢å®ƒ
                            process.terminate()
                            try:
                                # ç­‰å¾…è¿›ç¨‹ç»“æŸå¹¶è¯»å–è¾“å‡ºä»¥å…³é—­æ–‡ä»¶å¥æŸ„
                                stdout, stderr = process.communicate(timeout=5)
                            except subprocess.TimeoutExpired:
                                # å¦‚æœç­‰å¾…è¶…æ—¶ï¼Œå¼ºåˆ¶æ€æ­»è¿›ç¨‹
                                process.kill()
                                stdout, stderr = process.communicate()
                    except Exception as cleanup_error:
                        LOGGER.warning(f"Error during process cleanup for {job_name}: {cleanup_error}")
        
        try:
            # åœ¨å¯åŠ¨ç«¯å£è½¬å‘ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥ Pod çŠ¶æ€
            try:
                pods = await aio.to_thread(
                    self.core_v1.list_namespaced_pod,
                    namespace=self.config.Kubernetes.NAMESPACE,
                    label_selector=f"app={job_name}"
                )
                
                if not pods.items:
                    LOGGER.warning(f"No pods found for job {job_name}")
                    return False
                
                pod = pods.items[0]
                if pod.status.phase != "Running":
                    LOGGER.warning(f"Pod {pod.metadata.name} is not running (phase: {pod.status.phase})")
                    # ä»ç„¶å°è¯•åˆ›å»ºç«¯å£è½¬å‘ï¼Œä½†é¢„æœŸä¼šå¤±è´¥
                else:
                    # æ£€æŸ¥å®¹å™¨æ˜¯å¦å°±ç»ª
                    if pod.status.container_statuses:
                        ready_containers = [cs for cs in pod.status.container_statuses if cs.ready]
                        if not ready_containers:
                            LOGGER.warning(f"Pod {pod.metadata.name} is running but no containers are ready")
                    
            except Exception as e:
                LOGGER.warning(f"Failed to check pod status before port forward: {e}")
            
            # åˆ›å»ºåœæ­¢äº‹ä»¶
            stop_event = threading.Event()
            
            # å¯åŠ¨ç«¯å£è½¬å‘çº¿ç¨‹
            thread = threading.Thread(target=port_forward_worker, daemon=True)
            thread.start()
            
            # å­˜å‚¨ç«¯å£è½¬å‘ä¿¡æ¯
            self._port_forwards[job_name] = {
                'thread': thread,
                'local_port': local_port,
                'target_port': target_port,
                'service_name': f"{job_name}-svc",
                'stop_event': stop_event,
                'process': None  # å°†åœ¨çº¿ç¨‹ä¸­è®¾ç½®
            }
            
            # ç­‰å¾…ç«¯å£è½¬å‘å»ºç«‹ï¼Œä½†æ—¶é—´æ›´çŸ­
            for i in range(5):  # æœ€å¤šç­‰å¾… 5 ç§’
                await aio.sleep(1)
                if await self._test_local_port(local_port):
                    LOGGER.info(f"Port forward established for {job_name} on localhost:{local_port}")
                    return True
            
            # å¦‚æœ 5 ç§’åä»ä¸å¯ç”¨ï¼Œè®°å½•è­¦å‘Šä½†ä¸æ¸…ç†ï¼ˆå¯èƒ½ Pod è¿˜åœ¨å¯åŠ¨ï¼‰
            LOGGER.warning(f"Port forward not ready yet for {job_name} after 5s")
            return True  # è¿”å› Trueï¼Œè®©è°ƒç”¨è€…çŸ¥é“ç«¯å£è½¬å‘å·²å¯åŠ¨ï¼Œå³ä½¿è¿˜ä¸å¯ç”¨
            
        except Exception as e:
            LOGGER.error(f"Failed to create port forward for {job_name}: {e}")
            return False

    async def _test_local_port(self, port: int) -> bool:
        """æµ‹è¯•æœ¬åœ°ç«¯å£æ˜¯å¦å¯ç”¨"""
        try:
            future = aio.get_event_loop().run_in_executor(
                None, 
                self._sync_test_local_port, 
                port
            )
            return await future
        except Exception:
            return False

    def _sync_test_local_port(self, port: int) -> bool:
        """åŒæ­¥æµ‹è¯•æœ¬åœ°ç«¯å£"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(2)
                result = sock.connect_ex(('localhost', port))
                return result == 0
        except Exception:
            return False

    async def stop_port_forward(self, job_name: str) -> None:
        """åœæ­¢æŒ‡å®šä½œä¸šçš„ç«¯å£è½¬å‘"""
        if job_name in self._port_forwards:
            forward_info = self._port_forwards[job_name]
            
            # è®¾ç½®åœæ­¢äº‹ä»¶
            if 'stop_event' in forward_info:
                forward_info['stop_event'].set()
            
            # ç»ˆæ­¢è¿›ç¨‹å¹¶ç¡®ä¿æ–‡ä»¶å¥æŸ„è¢«å…³é—­
            if 'process' in forward_info and forward_info['process']:
                process = forward_info['process']
                try:
                    if process.poll() is None:
                        # è¿›ç¨‹ä»åœ¨è¿è¡Œï¼Œç»ˆæ­¢å®ƒ
                        process.terminate()
                        try:
                            # ç­‰å¾…è¿›ç¨‹ç»“æŸå¹¶è¯»å–è¾“å‡ºä»¥å…³é—­æ–‡ä»¶å¥æŸ„
                            stdout, stderr = process.communicate(timeout=5)
                            LOGGER.debug(f"Port forward process terminated gracefully for {job_name}")
                        except subprocess.TimeoutExpired:
                            # å¦‚æœç­‰å¾…è¶…æ—¶ï¼Œå¼ºåˆ¶æ€æ­»è¿›ç¨‹
                            process.kill()
                            stdout, stderr = process.communicate()
                            LOGGER.warning(f"Port forward process killed for {job_name}")
                    else:
                        # è¿›ç¨‹å·²ç»ç»“æŸï¼Œä½†ä»éœ€è¦è¯»å–è¾“å‡ºä»¥å…³é—­æ–‡ä»¶å¥æŸ„
                        try:
                            stdout, stderr = process.communicate(timeout=1)
                        except subprocess.TimeoutExpired:
                            # è¿™ç§æƒ…å†µä¸å¤ªå¯èƒ½å‘ç”Ÿï¼Œä½†ä»¥é˜²ä¸‡ä¸€
                            LOGGER.warning(f"Failed to read output from terminated process for {job_name}")
                except Exception as e:
                    LOGGER.warning(f"Error stopping port forward process for {job_name}: {e}")
            
            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            if 'thread' in forward_info and forward_info['thread'].is_alive():
                forward_info['thread'].join(timeout=5)
                if forward_info['thread'].is_alive():
                    LOGGER.warning(f"Port forward thread did not stop within timeout for {job_name}")
            
            del self._port_forwards[job_name]
            LOGGER.info(f"Stopped port forward for {job_name}")

    def _get_available_local_port(self) -> int:
        """è·å–å¯ç”¨çš„æœ¬åœ°ç«¯å£"""
        for port in range(self._local_port_base, self._local_port_base + 1000):
            if port not in [info['local_port'] for info in self._port_forwards.values()]:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                    try:
                        sock.bind(('localhost', port))
                        return port
                    except OSError:
                        continue
        raise ClusterError("No available local port found")

    def _build_labels(self, job_params: JobParams) -> Dict[str, str]:
        """æ„å»ºæ ‡ç­¾"""
        return {
            "managed-by": "yatcc-se",
            "user-id": str(job_params.user_id),
            "type": "code-server"
        }

    async def get_job_status(self, job_name: str) -> JobInfo.Status:
        """è·å–ä½œä¸šçŠ¶æ€"""
        await self.ensure_initialized()
        
        try:
            deployment = await aio.to_thread(
                self.apps_v1.read_namespaced_deployment,
                name=job_name,
                namespace=self.config.Kubernetes.NAMESPACE
            )
            
            # ç¡®å®šçŠ¶æ€
            if deployment.status.ready_replicas and deployment.status.ready_replicas >= 1:
                return JobInfo.Status.RUNNING
            elif deployment.status.unavailable_replicas:
                return JobInfo.Status.FAILED
            else:
                return JobInfo.Status.PENDING
        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"Job not found: {job_name}")
            raise ClusterError(f"Failed to get job status: {e}")

    async def get_job_info(self, job_name: str) -> JobInfo:
        """è·å–ä½œä¸šè¯¦ç»†ä¿¡æ¯"""
        await self.ensure_initialized()
        
        try:
            deployment = await aio.to_thread(
                self.apps_v1.read_namespaced_deployment,
                name=job_name,
                namespace=self.config.Kubernetes.NAMESPACE
            )
            
            container = deployment.spec.template.spec.containers[0]
            labels = deployment.metadata.labels or {}
            
            # ç¡®å®šçŠ¶æ€
            if deployment.status.ready_replicas and deployment.status.ready_replicas >= 1:
                status = JobInfo.Status.RUNNING
            elif deployment.status.unavailable_replicas:
                status = JobInfo.Status.FAILED
            else:
                status = JobInfo.Status.PENDING
            
            # è·å–æœåŠ¡ URL
            service_url = None
            try:
                service = await aio.to_thread(
                    self.core_v1.read_namespaced_service,
                    name=f"{job_name}-svc",
                    namespace=self.config.Kubernetes.NAMESPACE
                )
                service_url = f"http://{service.metadata.name}.{service.metadata.namespace}.svc.cluster.local:{self.config.Codespace.PORT}"
            except ApiException:
                pass
            
            return JobInfo(
                id=deployment.metadata.name,
                name=deployment.metadata.labels.get("app", deployment.metadata.name),
                image=container.image,
                ports=[self.config.Codespace.PORT],
                env={env_var.name: env_var.value for env_var in (container.env or [])},
                status=status,
                created_at=deployment.metadata.creation_timestamp.isoformat() if deployment.metadata.creation_timestamp else None,
                namespace=deployment.metadata.namespace,
                service_url=service_url,
                user_id=int(labels.get("user-id", 0)) if labels.get("user-id") else None,
            )
            
        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"Job not found: {job_name}")
            raise ClusterError(f"Failed to get job info: {e}")
    
    async def delete_job(self, job_name: str) -> List[str]:
        """æ ¹æ®ç”¨æˆ·IDä¸´æ—¶å…³é—­ç”¨æˆ·çš„æ‰€æœ‰ Deployment"""
        await self.ensure_initialized()
        
        try:
            # æŸ¥æ‰¾ç”¨æˆ·çš„æ‰€æœ‰ Deployment
            deployments = await aio.to_thread(
                self.apps_v1.list_namespaced_deployment,
                name =job_name,
                namespace=self.config.Kubernetes.NAMESPACE
            )
            
            suspended_jobs = []
            
            for deployment in deployments.items:
                job_name = deployment.metadata.name
                current_replicas = deployment.spec.replicas
                
                # åœæ­¢ç«¯å£è½¬å‘
                await self.stop_port_forward(job_name)
                
                # ä½¿ç”¨é‡è¯•æœºåˆ¶æ›´æ–° deployment
                success = await self._suspend_deployment_with_retry(job_name, current_replicas)
                if success:
                    suspended_jobs.append(job_name)
                    logger.info(f"Suspended deployment: {job_name} (user: {job_name})")
                else:
                    logger.warning(f"Failed to suspend deployment: {job_name}")
            
            logger.info(f"Suspended {len(suspended_jobs)} deployments for user {job_name}")
            return suspended_jobs
            
        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"No deployments found for user: {job_name}")
            raise ClusterError(f"Failed to suspend deployments: {e}")

    async def _suspend_deployment_with_retry(self, job_name: str, original_replicas: int) -> bool:
        """ä½¿ç”¨é‡è¯•æœºåˆ¶æš‚åœ deployment"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # é‡æ–°è¯»å–æœ€æ–°çš„ deployment
                deployment = await aio.to_thread(
                    self.apps_v1.read_namespaced_deployment,
                    name=job_name,
                    namespace=self.config.Kubernetes.NAMESPACE
                )
                
                # ä¿å­˜å½“å‰å‰¯æœ¬æ•°åˆ°æ³¨è§£ä¸­ï¼Œä»¥ä¾¿æ¢å¤
                annotations = deployment.metadata.annotations or {}
                annotations["yatcc-se/suspended"] = "true"
                annotations["yatcc-se/original-replicas"] = str(original_replicas)
                
                # æ›´æ–° Deploymentï¼šè®¾ç½®å‰¯æœ¬æ•°ä¸º 0 å¹¶æ·»åŠ æ³¨è§£
                deployment.spec.replicas = 0
                deployment.metadata.annotations = annotations
                
                await aio.to_thread(
                    self.apps_v1.patch_namespaced_deployment,
                    name=job_name,
                    namespace=self.config.Kubernetes.NAMESPACE,
                    body=deployment
                )
                
                return True
                
            except ApiException as e:
                if e.status == 409 and attempt < max_retries - 1:
                    # ç‰ˆæœ¬å†²çªï¼Œç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    logger.warning(f"Deployment {job_name} suspend conflict, retrying... (attempt {attempt + 1})")
                    await aio.sleep(0.5 * (attempt + 1))
                    continue
                elif e.status == 404:
                    logger.warning(f"Deployment {job_name} not found during suspend")
                    return False
                else:
                    logger.error(f"Failed to suspend deployment {job_name}: {e}")
                    return False
        
        logger.error(f"Failed to suspend deployment {job_name} after {max_retries} attempts")
        return False

    async def cleanup(self, job_name: str) -> None:
        """åˆ é™¤ä½œä¸š"""
        await self.ensure_initialized()
        
        try:
            await self.stop_port_forward(job_name)

            await self._cleanup_job_resources(job_name)
            LOGGER.info(f"Job deleted: {job_name}")
        except ApiException as e:
            if e.status != 404:
                raise ClusterError(f"Failed to delete job: {e}")

    async def _cleanup_job_resources(self, job_name: str):
        """æ¸…ç†ä½œä¸šç›¸å…³èµ„æº"""
        namespace = self.config.Kubernetes.NAMESPACE
        
        # åˆ é™¤ Deployment
        try:
            await aio.to_thread(
                self.apps_v1.delete_namespaced_deployment,
                name=job_name,
                namespace=namespace
            )
            LOGGER.info(f"Deleted Deployment: {job_name}")
        except ApiException as e:
            if e.status != 404:
                LOGGER.warning(f"Failed to delete deployment {job_name}: {e}")
        
        # åˆ é™¤ Service
        try:
            await aio.to_thread(
                self.core_v1.delete_namespaced_service,
                name=f"{job_name}-svc",
                namespace=namespace
            )
            LOGGER.info(f"Deleted Service: {job_name}-svc")
        except ApiException as e:
            if e.status != 404:
                LOGGER.warning(f"Failed to delete service {job_name}-svc: {e}")
        
        # åˆ é™¤ PVC
        # try:
        #     await aio.to_thread(
        #         self.core_v1.delete_namespaced_persistent_volume_claim,
        #         name=f"{job_name}-pvc",
        #         namespace=namespace
        #     )
        #     LOGGER.info(f"Deleted PVC: {job_name}-pvc")
        # except ApiException as e:
        #     if e.status != 404:
        #         LOGGER.warning(f"Failed to delete PVC {job_name}-pvc: {e}")

    async def list_jobs(self) -> List[JobInfo]:
        """åˆ—å‡ºæ‰€æœ‰ä½œä¸š"""
        await self.ensure_initialized()
        
        # æ„å»ºæ ‡ç­¾é€‰æ‹©å™¨
        label_selectors = ["managed-by=yatcc-se", "type=code-server"]
        
        label_selector = ",".join(label_selectors)
        
        try:
            deployments = await aio.to_thread(
                self.apps_v1.list_namespaced_deployment,
                namespace=self.config.Kubernetes.NAMESPACE,
                label_selector=label_selector
            )
            
            jobs = []
            for deployment in deployments.items:
                container = deployment.spec.template.spec.containers[0]
                labels = deployment.metadata.labels or {}
                
                # ç¡®å®šçŠ¶æ€
                if deployment.status.ready_replicas and deployment.status.ready_replicas >= 1:
                    status = JobInfo.Status.RUNNING
                elif deployment.status.unavailable_replicas:
                    status = JobInfo.Status.FAILED
                else:
                    status = JobInfo.Status.PENDING
                
                job_info = JobInfo(
                    id=deployment.metadata.name,
                    name=deployment.metadata.labels.get("app", deployment.metadata.name),
                    image=container.image,
                    ports=[self.config.Codespace.PORT],
                    env={env_var.name: env_var.value for env_var in (container.env or [])},
                    status=status,
                    created_at=deployment.metadata.creation_timestamp.isoformat() if deployment.metadata.creation_timestamp else None,
                    namespace=deployment.metadata.namespace,
                    user_id=int(labels.get("user-id", 0)) if labels.get("user-id") else None,
                )
                jobs.append(job_info)
            
            return jobs
            
        except ApiException as e:
            raise ClusterError(f"Failed to list jobs: {e}")

    async def get_job_logs(self, job_name: str, lines: int = 100) -> str:
        """è·å–ä½œä¸šæ—¥å¿—"""
        await self.ensure_initialized()
        
        try:
            # è·å–å…³è”çš„ Pod
            pods = await aio.to_thread(
                self.core_v1.list_namespaced_pod,
                namespace=self.config.Kubernetes.NAMESPACE,
                label_selector=f"app={job_name}"
            )
            
            if not pods.items:
                raise JobNotFoundError(f"No pods found for job: {job_name}")
            
            # è·å–ç¬¬ä¸€ä¸ª Pod çš„æ—¥å¿—
            pod = pods.items[0]
            logs = await aio.to_thread(
                self.core_v1.read_namespaced_pod_log,
                name=pod.metadata.name,
                namespace=self.config.Kubernetes.NAMESPACE,
                container="code-server",
                tail_lines=lines
            )
            
            return logs
            
        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"Job not found: {job_name}")
            raise ClusterError(f"Failed to get job logs: {e}")