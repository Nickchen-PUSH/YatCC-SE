"""Kubernetes é›†ç¾¤å®ç°

ä¸“é—¨é’ˆå¯¹ç”¨æˆ·ç‹¬ç«‹çš„ code-server é•œåƒçš„éƒ¨ç½²å’Œç®¡ç†ã€‚
"""

import asyncio as aio
import threading
import socket
import subprocess
from base.logger import logger
from datetime import datetime
from typing import Any, List, Optional, Dict
from kubernetes.client.rest import ApiException
from dataclasses import dataclass

from config import CONFIG, Config
from . import (
    ClusterABC,
    JobParams,
    JobInfo,
    ClusterError,
    JobNotFoundError,
)

LOGGER = logger(__spec__, __file__)


@dataclass
class KubernetesSpec:
    """Kubernetes è§„æ ¼é…ç½®"""

    def __init__(self, job_params: JobParams):
        self.job_params = job_params

    def _build_deployment(self) -> Dict[str, Any]:
        """æ„å»ºå®Œæ•´çš„ Deployment è§„æ ¼"""
        return {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": self.job_params.name,
                "labels": self._build_labels(),
                "namespace": CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            },
            "spec": self._build_spec(),
        }

    def _build_service(self) -> Dict[str, Any]:
        """æ„å»º Service è§„æ ¼"""
        return {
            "apiVersion": "v1",
            "kind": "Service",
            "metadata": {
                "name": self.job_params.name + "-svc",
                "labels": self._build_labels(),
                "namespace": CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            },
            "spec": {
                "selector": {"app": self.job_params.name, **self._build_labels()},
                "ports": [
                    {
                        "port": CONFIG.CLUSTER.Codespace.PORT,
                        "targetPort": CONFIG.CLUSTER.Codespace.PORT,
                        "protocol": "TCP",
                        "name": "http",
                    }
                ],
                "type": "NodePort",
            },
        }

    def _build_spec(self) -> Dict[str, Any]:
        return {
            "replicas": 1,
            "selector": {"matchLabels": {"app": self.job_params.name}},
            "template": {
                "metadata": {
                    "labels": {"app": self.job_params.name, **self._build_labels()}
                },
                "spec": self._build_pod_spec(),
            },
        }

    def _build_pod_spec(self) -> Dict[str, Any]:
        return {
            "containers": [self._build_container()],
            "volumes": self._build_volumes(),
            "restartPolicy": "Always",
        }

    def _build_container(self) -> Dict[str, Any]:
        return {
            "name": "code-server",
            "image": self.job_params.image,
            "ports": [{"containerPort": CONFIG.CLUSTER.Codespace.PORT}],
            "env": [{"name": k, "value": v} for k, v in self.job_params.env.items()],
            "args": [
                "--bind-addr=0.0.0.0:8080",
                "--auth=password",
                "--disable-telemetry",
                "/workspace",
            ],
            "volumeMounts": self._build_volume_mounts(),
            "resources": self._build_resources(),
            "readinessProbe": self._build_probe(),
            "livenessProbe": self._build_probe(initial_delay=60, period=30),
        }

    def _build_volume_mounts(self) -> List[Dict[str, str]]:
        return [
            {"name": "code", "mountPath": "/workspace"},
            {"name": "io", "mountPath": "/io"},
            {"name": "root", "mountPath": "/data"},
        ]

    def _build_volumes(self) -> List[Dict[str, Any]]:
        base_path = f"{CONFIG.CORE.students_dir}/{self.job_params.user_id}"
        return [
            {
                "name": name,
                "hostPath": {
                    "path": f"{base_path}/{name}",
                    "type": "DirectoryOrCreate",
                },
            }
            for name in ["code", "io", "root"]
        ]

    def _build_resources(self) -> Dict[str, Dict[str, str]]:
        return {
            "requests": {"memory": "256Mi", "cpu": "250m"},
            "limits": {
                "memory": self.job_params.memory_limit
                or CONFIG.CLUSTER.Codespace.DEFAULT_MEMORY_LIMIT,
                "cpu": self.job_params.cpu_limit
                or CONFIG.CLUSTER.Codespace.DEFAULT_CPU_LIMIT,
            },
        }

    def _build_probe(self, initial_delay: int = 30, period: int = 10) -> Dict[str, Any]:
        return {
            "httpGet": {"path": "/", "port": CONFIG.CLUSTER.Codespace.PORT},
            "initialDelaySeconds": initial_delay,
            "periodSeconds": period,
        }

    def _build_labels(self) -> Dict[str, str]:
        return {
            "managed-by": "yatcc-se",
            "user-id": self.job_params.user_id,
            "type": "code-server",
        }


class KubernetesCluster(ClusterABC):
    """Kubernetes é›†ç¾¤å®ç°"""

    def __init__(self):
        self._k8s_client = None
        self._apps_v1 = None
        self._core_v1 = None
        self._is_initialized = False
        self._port_forwards: Dict[str, Dict] = {}
        self._local_port_base = 30000  # æœ¬åœ°ç«¯å£åŸºæ•°ï¼Œç”¨äºç«¯å£è½¬å‘

    async def initialize(self):
        """åˆå§‹åŒ– Kubernetes é›†ç¾¤è¿æ¥"""
        if self._is_initialized:
            return

        try:
            # å¯¼å…¥ Kubernetes å®¢æˆ·ç«¯
            from kubernetes import client, config as k8s_config

            # å°è¯•åŠ è½½é…ç½®
            try:
                k8s_config.load_kube_config()
                LOGGER.info("Loaded kubeconfig from file")
            except Exception:
                try:
                    # å¦‚æœåœ¨é›†ç¾¤å†…ï¼Œå°è¯•åŠ è½½é›†ç¾¤å†…é…ç½®
                    k8s_config.load_incluster_config()
                    LOGGER.info("Loaded in-cluster config")
                except Exception as e:
                    raise ClusterError(f"Failed to load Kubernetes config: {e}")

            # åˆ›å»ºå®¢æˆ·ç«¯
            self._k8s_client = client.ApiClient()
            self._apps_v1 = client.AppsV1Api()
            self._core_v1 = client.CoreV1Api()

            # æµ‹è¯•è¿æ¥
            await aio.to_thread(self._core_v1.list_namespace, timeout_seconds=10)

            self._is_initialized = True
            LOGGER.info("Kubernetes cluster initialized successfully")

        except Exception as e:
            LOGGER.error(f"Failed to initialize Kubernetes cluster: {e}")
            raise ClusterError(f"Kubernetes initialization failed: {e}")

    async def ensure_initialized(self):
        """ç¡®ä¿å·²åˆå§‹åŒ–"""
        if not self._is_initialized:
            await self.initialize()

    @property
    def apps_v1(self):
        """è·å– AppsV1Api å®¢æˆ·ç«¯"""
        return self._apps_v1

    @property
    def core_v1(self):
        """è·å– CoreV1Api å®¢æˆ·ç«¯"""
        return self._core_v1

    async def allocate_resources(self, job_params):
        """åˆ›å»ºä½œä¸š"""
        await self.ensure_initialized()
        try:
            # åˆ›å»º Deployment
            deployment_spec = KubernetesSpec(job_params)._build_deployment()

            await aio.to_thread(
                self.apps_v1.create_namespaced_deployment,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                body=deployment_spec,
            )
            LOGGER.info(f"Created Deployment: {job_params.name}")

            # åˆ›å»º Service
            await self._create_service(job_params)
            LOGGER.info(f"Created Service for Deployment: {job_params.name}")

            service_url = await self.get_service_url(job_params.name)

            return JobInfo(
                id=job_params.name,
                name=job_params.name,
                image=job_params.image,
                ports=job_params.ports,
                env=job_params.env,
                status=JobInfo.Status.RUNNING,
                created_at=datetime.now().isoformat(),
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                service_url=service_url,
                user_id=job_params.user_id,
            )

        except Exception as e:
            LOGGER.error(f"Failed to submit job: {e}")
            # æ¸…ç†å¯èƒ½åˆ›å»ºçš„èµ„æº
            await self._cleanup_job_resources(job_params.name)
            raise ClusterError(f"Failed to submit job: {e}")

    async def submit_job(self, job_params: JobParams) -> JobInfo:
        """æäº¤ code-server ä½œä¸šåˆ° Kubernetes"""
        await self.ensure_initialized()

        # éªŒè¯å¿…éœ€å‚æ•°
        if not job_params.user_id:
            raise ValueError("job_user_id is required")

        existing_job = None
        deployments = await aio.to_thread(
            self.apps_v1.list_namespaced_deployment,
            namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            label_selector=f"managed-by=yatcc-se,user-id={job_params.user_id},type=code-server",
        )

        if deployments.items:
            deployment = deployments.items[0]
            job_name = deployment.metadata.name
            LOGGER.info(
                f"Found existing deployment for user {job_params.user_id}: {job_name}"
            )
            existing_job = job_name

        if existing_job:
            return await self._resume_user_deployment(existing_job, job_params)
        else:
            LOGGER.info(
                "No existing deployment found for user {job_params.user_id}, creating a new one."
            )
            return await self.allocate_resources(job_params)

    async def _resume_user_deployment(
        self, job_name: str, job_params: JobParams
    ) -> JobInfo:
        """æ¢å¤ç”¨æˆ·çš„ Deployment"""
        try:
            deployment = await aio.to_thread(
                self.apps_v1.read_namespaced_deployment,
                name=job_name,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            )

            # æ£€æŸ¥æ˜¯å¦è¢«æš‚åœ
            annotations = deployment.metadata.annotations or {}
            is_suspended = annotations.get("yatcc-se/suspended") == "true"

            LOGGER.info(f"Checking deployment {job_name} suspension status:")
            LOGGER.info(f"  - Annotations: {annotations}")
            LOGGER.info(f"  - Is suspended: {is_suspended}")

            if is_suspended:
                # æ¢å¤ Deployment
                LOGGER.info(f"Unsuspending deployment: {job_name}")
                await self._unsuspend_deployment(job_name)
                LOGGER.info(f"Resumed suspended deployment: {job_name}")

                # ç­‰å¾…ä¸€ä¸‹ï¼Œç„¶åé‡æ–°è¯»å– deployment ä»¥è·å–æœ€æ–°çŠ¶æ€
                await aio.sleep(2)
                deployment = await aio.to_thread(
                    self.apps_v1.read_namespaced_deployment,
                    name=job_name,
                    namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                )

                # éªŒè¯æ³¨è§£æ˜¯å¦è¢«ç§»é™¤
                updated_annotations = deployment.metadata.annotations or {}
                LOGGER.info(f"After unsuspend, annotations: {updated_annotations}")
            else:
                # Deployment å·²ç»åœ¨è¿è¡Œ
                LOGGER.info(f"Deployment {job_name} is already running")

            # ğŸ¯ æ›´æ–°ç¯å¢ƒå˜é‡å’Œé…ç½®
            await self._update_deploy_spec(job_params)

            # ç¡®ä¿ Service å­˜åœ¨
            service_url = await self._check_service(job_params)

            # å†æ¬¡è¯»å–æœ€æ–°çš„ deployment çŠ¶æ€
            final_deployment = await aio.to_thread(
                self.apps_v1.read_namespaced_deployment,
                name=job_name,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            )

            # è¿”å› JobInfo
            container = final_deployment.spec.template.spec.containers[0]
            labels = final_deployment.metadata.labels or {}

            # ç¡®å®šçŠ¶æ€
            if (
                final_deployment.status.ready_replicas
                and final_deployment.status.ready_replicas >= 1
            ):
                status = JobInfo.Status.RUNNING
            elif final_deployment.status.unavailable_replicas:
                status = JobInfo.Status.FAILED
            else:
                status = JobInfo.Status.PENDING
            user_id = labels.get("user-id", 0)
            if user_id is not None:
                user_id = str(user_id)
            return JobInfo(
                id=final_deployment.metadata.name,
                name=final_deployment.metadata.name,
                image=container.image,
                ports=[CONFIG.CLUSTER.Codespace.PORT],
                env={env_var.name: env_var.value for env_var in (container.env or [])},
                status=status,
                created_at=(
                    final_deployment.metadata.creation_timestamp.isoformat()
                    if final_deployment.metadata.creation_timestamp
                    else None
                ),
                namespace=final_deployment.metadata.namespace,
                service_url=service_url,
                user_id=user_id,
            )

        except ApiException as e:
            raise ClusterError(f"Failed to resume deployment: {e}")

    async def _unsuspend_deployment(self, job_name: str) -> None:
        """æ¢å¤è¢«æš‚åœçš„ Deployment"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                LOGGER.info(
                    f"Unsuspending deployment {job_name} (attempt {attempt + 1})"
                )

                # é‡æ–°è¯»å–æœ€æ–°çš„ deployment
                deployment = await aio.to_thread(
                    self.apps_v1.read_namespaced_deployment,
                    name=job_name,
                    namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                )

                annotations = deployment.metadata.annotations or {}
                LOGGER.info(f"Current annotations before unsuspend: {annotations}")

                # ä¿®å¤ï¼šæ­£ç¡®è·å–åŸå§‹å‰¯æœ¬æ•°
                original_replicas_str = annotations.get(
                    "yatcc-se/original-replicas", "1"
                )
                try:
                    original_replicas = int(original_replicas_str)
                except ValueError:
                    LOGGER.warning(
                        f"Invalid original-replicas value: {original_replicas_str}, using default 1"
                    )
                    original_replicas = 1

                # æ¢å¤å‰¯æœ¬æ•°
                deployment.spec.replicas = original_replicas
                LOGGER.info(f"Setting replicas to: {original_replicas}")

                # ğŸ¯ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ³¨è§£è¢«å®Œå…¨åˆ é™¤
                # åˆ›å»ºæ–°çš„æ³¨è§£å­—å…¸ï¼Œæ’é™¤æš‚åœç›¸å…³çš„æ³¨è§£
                new_annotations = {}
                for key, value in annotations.items():
                    if key not in ["yatcc-se/suspended", "yatcc-se/original-replicas"]:
                        new_annotations[key] = value

                # è®¾ç½®æ–°çš„æ³¨è§£å­—å…¸
                deployment.metadata.annotations = new_annotations

                LOGGER.info(f"New annotations after removal: {new_annotations}")
                LOGGER.info(
                    f"Removed annotations: yatcc-se/suspended, yatcc-se/original-replicas"
                )

                # æ‰§è¡Œæ›´æ–°
                updated_deployment = await aio.to_thread(
                    self.apps_v1.patch_namespaced_deployment,
                    name=job_name,
                    namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                    body=deployment,
                )

                LOGGER.info(f"Successfully unsuspended deployment: {job_name}")
                LOGGER.info(f"Final replicas: {updated_deployment.spec.replicas}")
                LOGGER.info(
                    f"Final annotations: {updated_deployment.metadata.annotations}"
                )
                return

            except ApiException as e:
                if e.status == 409 and attempt < max_retries - 1:
                    # ç‰ˆæœ¬å†²çªï¼Œç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    LOGGER.warning(
                        f"Deployment {job_name} version conflict, retrying... (attempt {attempt + 1})"
                    )
                    await aio.sleep(0.5 * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´
                    continue
                elif e.status == 404:
                    LOGGER.warning(f"Deployment {job_name} not found during unsuspend")
                    return
                else:
                    raise ClusterError(f"Failed to unsuspend deployment: {e}")
            except Exception as e:
                LOGGER.error(f"Unexpected error unsuspending {job_name}: {e}")
                raise ClusterError(f"Unexpected error unsuspending deployment: {e}")

    async def _update_deploy_spec(self, job_params: JobParams) -> bool:
        """æ›´æ–° Deployment çš„é…ç½®"""
        job_name = job_params.name
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # é‡æ–°è¯»å–æœ€æ–°çš„ deployment
                deployment = await aio.to_thread(
                    self.apps_v1.read_namespaced_deployment,
                    name=job_name,
                    namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                )

                container = deployment.spec.template.spec.containers[0]
                # æ›´æ–°ç¯å¢ƒå˜é‡
                container.env = [
                    {"name": k, "value": v} for k, v in job_params.env.items()
                ]

                # æ›´æ–°èµ„æºé™åˆ¶
                container.resources.limits = {
                    "memory": job_params.memory_limit
                    or CONFIG.CLUSTER.Codespace.DEFAULT_MEMORY_LIMIT,
                    "cpu": job_params.cpu_limit
                    or CONFIG.CLUSTER.Codespace.DEFAULT_CPU_LIMIT,
                }

                await aio.to_thread(
                    self.apps_v1.patch_namespaced_deployment,
                    name=job_name,
                    namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                    body=deployment,
                )

                LOGGER.info(f"Updated deployment configuration: {job_name}")
                return True
            except ApiException as e:
                if e.status == 409 and attempt < max_retries - 1:
                    # ç‰ˆæœ¬å†²çªï¼Œç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    LOGGER.warning(
                        f"Deployment {job_name} update conflict, retrying... (attempt {attempt + 1})"
                    )
                    await aio.sleep(0.5 * (attempt + 1))
                    continue
                elif e.status == 404:
                    LOGGER.warning(f"Deployment {job_name} not found during update")
                    return False
                else:
                    LOGGER.warning(f"Failed to update deployment {job_name}: {e}")
                    return False

        LOGGER.warning(
            f"Failed to update deployment {job_name} after {max_retries} attempts"
        )
        return False

    async def _check_service(self, job_params: JobParams) -> str:
        """ç¡®ä¿ Service å­˜åœ¨"""
        svc_name = job_params.name + "-svc"

        try:
            service = await aio.to_thread(
                self.core_v1.read_namespaced_service,
                name=svc_name,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            )

            # Service å­˜åœ¨ï¼Œè¿”å› URL
            return f"http://{service.metadata.name}.{service.metadata.namespace}.svc.cluster.local:{service.spec.ports[0].port}"

        except ApiException as e:
            if e.status == 404:
                # Service ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                LOGGER.info(f"Service not found for {svc_name}, creating...")
                return await self._create_service(job_params)
            raise ClusterError(f"Failed to check service: {e}")

    async def _create_service(self, job_params: JobParams) -> str:
        """åˆ›å»º Serviceï¼Œè¿”å›å†…éƒ¨é›†ç¾¤ URL"""
        service_spec = KubernetesSpec(job_params)._build_service()
        svc_name = service_spec["metadata"]["name"]
        service = await aio.to_thread(
            self.core_v1.create_namespaced_service,
            namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            body=service_spec,
        )

        node_port = service.spec.ports[0].node_port
        service.nodeport = node_port

        # è¿”å›å†…éƒ¨é›†ç¾¤ URL
        service_url = f"http://{svc_name}.{CONFIG.CLUSTER.Kubernetes.NAMESPACE}.svc.cluster.local:{CONFIG.CLUSTER.Codespace.PORT}"
        LOGGER.info(f"Created Service: {svc_name}, Internal URL: {service_url}")
        return service_url

    async def get_service_url(self, job_name: str) -> str:
        """è·å–æŒ‡å®šä½œä¸šçš„æœåŠ¡è®¿é—® URL - åˆ›å»ºç«¯å£è½¬å‘å¹¶è¿”å›æœ¬åœ° URL"""
        await self.ensure_initialized()
        try:
            # å…ˆæ£€æŸ¥ Service æ˜¯å¦å­˜åœ¨
            service = await aio.to_thread(
                self.core_v1.read_namespaced_service,
                name=f"{job_name}-svc",
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            )

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç«¯å£è½¬å‘
            if job_name in self._port_forwards:
                forward_info = self._port_forwards[job_name]
                if forward_info["thread"].is_alive():
                    local_url = f"http://localhost:{forward_info['local_port']}"
                    LOGGER.debug(
                        f"Using existing port forward for {job_name}: {local_url}"
                    )
                    return local_url
                else:
                    # æ¸…ç†æ— æ•ˆçš„ç«¯å£è½¬å‘
                    del self._port_forwards[job_name]

            # åˆ›å»ºæ–°çš„ç«¯å£è½¬å‘
            local_port = self._get_available_local_port()
            target_port = service.spec.ports[0].port

            # ä½¿ç”¨ kubectl port-forward åˆ›å»ºç«¯å£è½¬å‘
            success = await self._create_kubectl_port_forward(
                job_name=job_name, local_port=local_port, target_port=target_port
            )

            if success:
                local_url = f"http://localhost:{local_port}"
                LOGGER.info(
                    f"Created port forward for {job_name}: {local_url} -> {target_port}"
                )
                return local_url
            else:
                raise ClusterError(f"Failed to create port forward for {job_name}")

        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"Service not found for job: {job_name}")
            raise ClusterError(f"Failed to get service URL: {e}")

    async def _create_kubectl_port_forward(
        self, job_name: str, local_port: int, target_port: int
    ) -> bool:
        """ä½¿ç”¨ kubectl port-forward åˆ›å»ºç«¯å£è½¬å‘"""
        import subprocess

        def port_forward_worker():
            process = None
            try:
                service_name = f"{job_name}-svc"

                # æ„å»º kubectl port-forward å‘½ä»¤
                cmd = [
                    "kubectl",
                    "port-forward",
                    f"svc/{service_name}",
                    f"{local_port}:{target_port}",
                    "-n",
                    CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                ]

                LOGGER.info(f"Starting port forward: {' '.join(cmd)}")

                # å¯åŠ¨ç«¯å£è½¬å‘è¿›ç¨‹
                process = subprocess.Popen(
                    cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
                )

                # ä¿å­˜è¿›ç¨‹å¼•ç”¨
                forward_info = self._port_forwards.get(job_name, {})
                forward_info["process"] = process
                self._port_forwards[job_name] = forward_info

                # ç­‰å¾…è¿›ç¨‹ç»“æŸæˆ–åœæ­¢ä¿¡å·
                stop_event = forward_info.get("stop_event", threading.Event())

                # è¯»å–ä¸€äº›åˆå§‹è¾“å‡ºæ¥æ£€æŸ¥æ˜¯å¦æˆåŠŸå¯åŠ¨
                import select
                import time

                initial_output_read = False

                while not stop_event.is_set():
                    if process.poll() is not None:
                        # è¿›ç¨‹å·²ç»“æŸï¼Œè¯»å–æ‰€æœ‰è¾“å‡ºå¹¶å…³é—­æ–‡ä»¶å¥æŸ„
                        try:
                            stdout, stderr = process.communicate(timeout=5)
                            if stderr:
                                LOGGER.error(
                                    f"Port forward error for {job_name}: {stderr}"
                                )
                                if "pod is not running" in stderr:
                                    LOGGER.warning(
                                        f"Pod for {job_name} is not running yet"
                                    )
                                elif "unable to forward port" in stderr:
                                    LOGGER.warning(
                                        f"Unable to forward port for {job_name}"
                                    )
                        except subprocess.TimeoutExpired:
                            # å¼ºåˆ¶ç»ˆæ­¢å¹¶è¯»å–è¾“å‡º
                            process.kill()
                            stdout, stderr = process.communicate()
                            LOGGER.warning(
                                f"Port forward process killed for {job_name}"
                            )
                        break

                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å¾ªç¯ï¼Œå°è¯•è¯»å–ä¸€äº›åˆå§‹è¾“å‡º
                    if not initial_output_read:
                        time.sleep(2)  # ç»™è¿›ç¨‹ä¸€äº›å¯åŠ¨æ—¶é—´
                        initial_output_read = True

                    time.sleep(1)

            except Exception as e:
                LOGGER.error(f"Port forward worker error for {job_name}: {e}")
            finally:
                # ç¡®ä¿è¿›ç¨‹è¢«æ­£ç¡®æ¸…ç†
                if process is not None:
                    try:
                        if process.poll() is None:
                            # è¿›ç¨‹ä»åœ¨è¿è¡Œï¼Œç»ˆæ­¢å®ƒ
                            process.terminate()
                            try:
                                # ç­‰å¾…è¿›ç¨‹ç»“æŸå¹¶è¯»å–è¾“å‡ºä»¥å…³é—­æ–‡ä»¶å¥æŸ„
                                stdout, stderr = process.communicate(timeout=5)
                            except subprocess.TimeoutExpired:
                                # å¦‚æœç­‰å¾…è¶…æ—¶ï¼Œå¼ºåˆ¶æ€æ­»è¿›ç¨‹
                                process.kill()
                                stdout, stderr = process.communicate()
                    except Exception as cleanup_error:
                        LOGGER.warning(
                            f"Error during process cleanup for {job_name}: {cleanup_error}"
                        )

        try:
            # åœ¨å¯åŠ¨ç«¯å£è½¬å‘ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥ Pod çŠ¶æ€
            try:
                pods = await aio.to_thread(
                    self.core_v1.list_namespaced_pod,
                    namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                    label_selector=f"app={job_name}",
                )

                if not pods.items:
                    LOGGER.warning(f"No pods found for job {job_name}")
                    return False

                pod = pods.items[0]
                if pod.status.phase != "Running":
                    LOGGER.warning(
                        f"Pod {pod.metadata.name} is not running (phase: {pod.status.phase})"
                    )
                    # ä»ç„¶å°è¯•åˆ›å»ºç«¯å£è½¬å‘ï¼Œä½†é¢„æœŸä¼šå¤±è´¥
                else:
                    # æ£€æŸ¥å®¹å™¨æ˜¯å¦å°±ç»ª
                    if pod.status.container_statuses:
                        ready_containers = [
                            cs for cs in pod.status.container_statuses if cs.ready
                        ]
                        if not ready_containers:
                            LOGGER.warning(
                                f"Pod {pod.metadata.name} is running but no containers are ready"
                            )

            except Exception as e:
                LOGGER.warning(f"Failed to check pod status before port forward: {e}")

            # åˆ›å»ºåœæ­¢äº‹ä»¶
            stop_event = threading.Event()

            # å¯åŠ¨ç«¯å£è½¬å‘çº¿ç¨‹
            thread = threading.Thread(target=port_forward_worker, daemon=True)
            thread.start()

            # å­˜å‚¨ç«¯å£è½¬å‘ä¿¡æ¯
            self._port_forwards[job_name] = {
                "thread": thread,
                "local_port": local_port,
                "target_port": target_port,
                "service_name": f"{job_name}-svc",
                "stop_event": stop_event,
                "process": None,  # å°†åœ¨çº¿ç¨‹ä¸­è®¾ç½®
            }

            # ç­‰å¾…ç«¯å£è½¬å‘å»ºç«‹ï¼Œä½†æ—¶é—´æ›´çŸ­
            for i in range(5):  # æœ€å¤šç­‰å¾… 5 ç§’
                await aio.sleep(1)
                if await self._test_local_port(local_port):
                    LOGGER.info(
                        f"Port forward established for {job_name} on localhost:{local_port}"
                    )
                    return True

            # å¦‚æœ 5 ç§’åä»ä¸å¯ç”¨ï¼Œè®°å½•è­¦å‘Šä½†ä¸æ¸…ç†ï¼ˆå¯èƒ½ Pod è¿˜åœ¨å¯åŠ¨ï¼‰
            LOGGER.warning(f"Port forward not ready yet for {job_name} after 5s")
            return True  # è¿”å› Trueï¼Œè®©è°ƒç”¨è€…çŸ¥é“ç«¯å£è½¬å‘å·²å¯åŠ¨ï¼Œå³ä½¿è¿˜ä¸å¯ç”¨

        except Exception as e:
            LOGGER.error(f"Failed to create port forward for {job_name}: {e}")
            return False

    async def _test_local_port(self, port: int) -> bool:
        """æµ‹è¯•æœ¬åœ°ç«¯å£æ˜¯å¦å¯ç”¨"""
        try:
            future = aio.get_event_loop().run_in_executor(
                None, self._sync_test_local_port, port
            )
            return await future
        except Exception:
            return False

    def _sync_test_local_port(self, port: int) -> bool:
        """åŒæ­¥æµ‹è¯•æœ¬åœ°ç«¯å£"""
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                sock.settimeout(2)
                result = sock.connect_ex(("localhost", port))
                return result == 0
        except Exception:
            return False

    async def stop_port_forward(self, job_name: str) -> None:
        """åœæ­¢æŒ‡å®šä½œä¸šçš„ç«¯å£è½¬å‘"""
        if job_name in self._port_forwards:
            forward_info = self._port_forwards[job_name]

            # è®¾ç½®åœæ­¢äº‹ä»¶
            if "stop_event" in forward_info:
                forward_info["stop_event"].set()

            # ç»ˆæ­¢è¿›ç¨‹å¹¶ç¡®ä¿æ–‡ä»¶å¥æŸ„è¢«å…³é—­
            if "process" in forward_info and forward_info["process"]:
                process = forward_info["process"]
                try:
                    if process.poll() is None:
                        # è¿›ç¨‹ä»åœ¨è¿è¡Œï¼Œç»ˆæ­¢å®ƒ
                        process.terminate()
                        try:
                            # ç­‰å¾…è¿›ç¨‹ç»“æŸå¹¶è¯»å–è¾“å‡ºä»¥å…³é—­æ–‡ä»¶å¥æŸ„
                            stdout, stderr = process.communicate(timeout=5)
                            LOGGER.debug(
                                f"Port forward process terminated gracefully for {job_name}"
                            )
                        except subprocess.TimeoutExpired:
                            # å¦‚æœç­‰å¾…è¶…æ—¶ï¼Œå¼ºåˆ¶æ€æ­»è¿›ç¨‹
                            process.kill()
                            stdout, stderr = process.communicate()
                            LOGGER.warning(
                                f"Port forward process killed for {job_name}"
                            )
                    else:
                        # è¿›ç¨‹å·²ç»ç»“æŸï¼Œä½†ä»éœ€è¦è¯»å–è¾“å‡ºä»¥å…³é—­æ–‡ä»¶å¥æŸ„
                        try:
                            stdout, stderr = process.communicate(timeout=1)
                        except subprocess.TimeoutExpired:
                            # è¿™ç§æƒ…å†µä¸å¤ªå¯èƒ½å‘ç”Ÿï¼Œä½†ä»¥é˜²ä¸‡ä¸€
                            LOGGER.warning(
                                f"Failed to read output from terminated process for {job_name}"
                            )
                except Exception as e:
                    LOGGER.warning(
                        f"Error stopping port forward process for {job_name}: {e}"
                    )

            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            if "thread" in forward_info and forward_info["thread"].is_alive():
                forward_info["thread"].join(timeout=5)
                if forward_info["thread"].is_alive():
                    LOGGER.warning(
                        f"Port forward thread did not stop within timeout for {job_name}"
                    )

            del self._port_forwards[job_name]
            LOGGER.info(f"Stopped port forward for {job_name}")

    def _get_available_local_port(self) -> int:
        """è·å–å¯ç”¨çš„æœ¬åœ°ç«¯å£"""
        for port in range(self._local_port_base, self._local_port_base + 1000):
            if port not in [
                info["local_port"] for info in self._port_forwards.values()
            ]:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
                    try:
                        sock.bind(("localhost", port))
                        return port
                    except OSError:
                        continue
        raise ClusterError("No available local port found")

    async def get_job_status(self, job_name: str) -> JobInfo.Status:
        """è·å–ä½œä¸šçŠ¶æ€"""
        await self.ensure_initialized()
        try:
            deployment = await aio.to_thread(
                self.apps_v1.read_namespaced_deployment,
                name=job_name,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            )

            # ç¡®å®šçŠ¶æ€
            if (
                deployment.status.ready_replicas
                and deployment.status.ready_replicas >= 1
            ):
                return JobInfo.Status.RUNNING
            elif deployment.status.unavailable_replicas:
                return JobInfo.Status.FAILED
            else:
                return JobInfo.Status.PENDING
        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"Job not found: {job_name}")
            raise ClusterError(f"Failed to get job status: {e}")

    async def get_job_info(self, job_name: str) -> JobInfo:
        """è·å–ä½œä¸šè¯¦ç»†ä¿¡æ¯"""
        await self.ensure_initialized()
        try:
            deployment = await aio.to_thread(
                self.apps_v1.read_namespaced_deployment,
                name=job_name,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            )

            container = deployment.spec.template.spec.containers[0]
            labels = deployment.metadata.labels or {}

            # ç¡®å®šçŠ¶æ€
            if (
                deployment.status.ready_replicas
                and deployment.status.ready_replicas >= 1
            ):
                status = JobInfo.Status.RUNNING
            elif deployment.status.unavailable_replicas:
                status = JobInfo.Status.FAILED
            else:
                status = JobInfo.Status.PENDING

            # è·å–æœåŠ¡ URL
            service_url = None
            try:
                service = await aio.to_thread(
                    self.core_v1.read_namespaced_service,
                    name=f"{job_name}-svc",
                    namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                )
                service_url = f"http://{service.metadata.name}.{service.metadata.namespace}.svc.cluster.local:{CONFIG.CLUSTER.Codespace.PORT}"
            except ApiException:
                pass

            user_id = labels.get("user-id")
            if user_id is not None:
                user_id = str(user_id)

            return JobInfo(
                id=deployment.metadata.name,
                name=deployment.metadata.name,
                image=container.image,
                ports=[CONFIG.CLUSTER.Codespace.PORT],
                env={env_var.name: env_var.value for env_var in (container.env or [])},
                status=status,
                created_at=(
                    deployment.metadata.creation_timestamp.isoformat()
                    if deployment.metadata.creation_timestamp
                    else None
                ),
                namespace=deployment.metadata.namespace,
                service_url=service_url,
                user_id=user_id,
            )

        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"Job not found: {job_name}")
            raise ClusterError(f"Failed to get job info: {e}")

    async def delete_job(self, job_name: str) -> List[str]:
        """æ ¹æ®ä½œä¸šåç§°æš‚åœæŒ‡å®šçš„ Deployment"""
        await self.ensure_initialized()
        try:
            # ç›´æ¥å°è¯•è¯»å–æŒ‡å®šåç§°çš„ deployment
            deployment = await aio.to_thread(
                self.apps_v1.read_namespaced_deployment,
                name=job_name,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
            )

            deployment_name = deployment.metadata.name
            current_replicas = deployment.spec.replicas or 1

            # åœæ­¢ç«¯å£è½¬å‘
            await self.stop_port_forward(deployment_name)

            # ä½¿ç”¨é‡è¯•æœºåˆ¶æ›´æ–° deployment
            success = await self._suspend_deployment(deployment_name, current_replicas)
            if success:
                LOGGER.info(f"Suspended deployment: {deployment_name}")
                return [deployment_name]
            else:
                LOGGER.warning(f"Failed to suspend deployment: {deployment_name}")
                raise ClusterError(f"Failed to suspend deployment: {deployment_name}")

        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"Job not found: {job_name}")
            raise ClusterError(f"Failed to suspend deployment: {e}")

    async def _suspend_deployment(self, job_name: str, original_replicas: int) -> bool:
        """ä½¿ç”¨é‡è¯•æœºåˆ¶æš‚åœ deployment"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # é‡æ–°è¯»å–æœ€æ–°çš„ deployment
                deployment = await aio.to_thread(
                    self.apps_v1.read_namespaced_deployment,
                    name=job_name,
                    namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                )

                # ä¿å­˜å½“å‰å‰¯æœ¬æ•°åˆ°æ³¨è§£ä¸­ï¼Œä»¥ä¾¿æ¢å¤
                annotations = deployment.metadata.annotations or {}
                annotations["yatcc-se/suspended"] = "true"
                annotations["yatcc-se/original-replicas"] = str(original_replicas)

                # æ›´æ–° Deploymentï¼šè®¾ç½®å‰¯æœ¬æ•°ä¸º 0 å¹¶æ·»åŠ æ³¨è§£
                deployment.spec.replicas = 0
                deployment.metadata.annotations = annotations

                await aio.to_thread(
                    self.apps_v1.patch_namespaced_deployment,
                    name=job_name,
                    namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                    body=deployment,
                )

                return True

            except ApiException as e:
                if e.status == 409 and attempt < max_retries - 1:
                    # ç‰ˆæœ¬å†²çªï¼Œç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    LOGGER.warning(
                        f"Deployment {job_name} suspend conflict, retrying... (attempt {attempt + 1})"
                    )
                    await aio.sleep(0.5 * (attempt + 1))
                    continue
                elif e.status == 404:
                    LOGGER.warning(f"Deployment {job_name} not found during suspend")
                    return False
                else:
                    LOGGER.error(f"Failed to suspend deployment {job_name}: {e}")
                    return False

        LOGGER.error(
            f"Failed to suspend deployment {job_name} after {max_retries} attempts"
        )
        return False

    async def cleanup_resources(self, job_name: str) -> None:
        """é‡Šæ”¾ä½œä¸š"""
        await self.ensure_initialized()
        try:
            await self.stop_port_forward(job_name)

            await self._cleanup_job_resources(job_name)
            LOGGER.info(f"Job deleted: {job_name}")
        except ApiException as e:
            if e.status != 404:
                raise ClusterError(f"Failed to delete job: {e}")

    async def _cleanup_job_resources(self, job_name: str):
        """æ¸…ç†ä½œä¸šç›¸å…³èµ„æº"""
        namespace = CONFIG.CLUSTER.Kubernetes.NAMESPACE

        # åˆ é™¤ Deployment
        try:
            await aio.to_thread(
                self.apps_v1.delete_namespaced_deployment,
                name=job_name,
                namespace=namespace,
            )
            LOGGER.info(f"Deleted Deployment: {job_name}")
        except ApiException as e:
            if e.status != 404:
                LOGGER.warning(f"Failed to delete deployment {job_name}: {e}")

        # åˆ é™¤ Service
        try:
            await aio.to_thread(
                self.core_v1.delete_namespaced_service,
                name=f"{job_name}-svc",
                namespace=namespace,
            )
            LOGGER.info(f"Deleted Service: {job_name}-svc")
        except ApiException as e:
            if e.status != 404:
                LOGGER.warning(f"Failed to delete service {job_name}-svc: {e}")

    async def list_jobs(self) -> List[JobInfo]:
        """åˆ—å‡ºæ‰€æœ‰ä½œä¸š"""
        await self.ensure_initialized()

        # æ„å»ºæ ‡ç­¾é€‰æ‹©å™¨
        label_selectors = ["managed-by=yatcc-se", "type=code-server"]

        label_selector = ",".join(label_selectors)

        try:
            deployments = await aio.to_thread(
                self.apps_v1.list_namespaced_deployment,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                label_selector=label_selector,
            )

            jobs = []
            for deployment in deployments.items:
                container = deployment.spec.template.spec.containers[0]
                labels = deployment.metadata.labels or {}

                # ç¡®å®šçŠ¶æ€
                if (
                    deployment.status.ready_replicas
                    and deployment.status.ready_replicas >= 1
                ):
                    status = JobInfo.Status.RUNNING
                elif deployment.status.unavailable_replicas:
                    status = JobInfo.Status.FAILED
                else:
                    status = JobInfo.Status.PENDING

                user_id = labels.get("user-id")
                if user_id is not None:
                    user_id = str(user_id)

                job_info = JobInfo(
                    id=deployment.metadata.name,
                    name=deployment.metadata.name,
                    image=container.image,
                    ports=[CONFIG.CLUSTER.Codespace.PORT],
                    env={
                        env_var.name: env_var.value for env_var in (container.env or [])
                    },
                    status=status,
                    created_at=(
                        deployment.metadata.creation_timestamp.isoformat()
                        if deployment.metadata.creation_timestamp
                        else None
                    ),
                    namespace=deployment.metadata.namespace,
                    user_id=user_id,
                )
                jobs.append(job_info)

            return jobs

        except ApiException as e:
            raise ClusterError(f"Failed to list jobs: {e}")

    async def get_job_logs(self, job_name: str, lines: int = 100) -> str:
        """è·å–ä½œä¸šæ—¥å¿—"""
        await self.ensure_initialized()
        try:
            # è·å–å…³è”çš„ Pod
            pods = await aio.to_thread(
                self.core_v1.list_namespaced_pod,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                label_selector=f"app={job_name}",
            )

            if not pods.items:
                raise JobNotFoundError(f"No pods found for job: {job_name}")

            # è·å–ç¬¬ä¸€ä¸ª Pod çš„æ—¥å¿—
            pod = pods.items[0]
            logs = await aio.to_thread(
                self.core_v1.read_namespaced_pod_log,
                name=pod.metadata.name,
                namespace=CONFIG.CLUSTER.Kubernetes.NAMESPACE,
                container="code-server",
                tail_lines=lines,
            )

            return logs

        except ApiException as e:
            if e.status == 404:
                raise JobNotFoundError(f"Job not found: {job_name}")
            raise ClusterError(f"Failed to get job logs: {e}")
